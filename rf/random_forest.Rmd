---
title: "Random Forest Disturbance Classification"
output:
  html_document:
    theme: default
    highlight: pygments
    toc: true
    toc_depth: 4
    toc_float: true
editor_options:
  chunk_output_type: console
---
author: Dan Wexler\
date: `r Sys.Date()`
```{r setup,include=FALSE}
# file settings
knitr::opts_chunk$set(include=TRUE,echo=TRUE)
```
#### Setup
```{r more setup,message=FALSE}
# loads data and libraries
setwd("C:/Users/dwexler/Desktop/R/nps-repo/rf")
x <- read.csv("rf_data.csv")
y <- read.csv("rf_labels.csv")
suppressWarnings({library(randomForest)})
suppressWarnings({library(knitr)})
suppressWarnings({library(randomcoloR)})
suppressWarnings({library(ggplot2)})
suppressWarnings({library(tidyr)})
```
#### Cleaning Data
```{r cleaning data}
# removes patches without a labeled change type
x <- x[y$ChangeType!="",]
y <- y[y$ChangeType!="",]
# filters by variable and converts x data to numeric
variables <- c("durMn","durSd","idxMagMn","idxMagSd","tcbMagMn","tcbMagSd",
               "tcgMagMn","tcgMagSd","tcwMagMn","tcwMagSd","tcbPreMn",
               "tcbPreSd","tcgPreMn","tcgPreSd","tcwPreMn","tcwPreSd",
               "tcbPstMn","tcbPstSd","tcgPstMn","tcgPstSd","tcwPstMn",
               "tcwPstSd","area","perim","shape_1","tcbPst01Mn","tcbPst01Sd",
               "tcbPst03Mn","tcbPst03Sd","tcbPst07Mn","tcbPst07Sd",
               "tcbPst15Mn","tcbPst15Sd","tcgPst01Mn","tcgPst01Sd",
               "tcgPst03Mn","tcgPst03Sd","tcgPst07Mn","tcgPst07Sd",
               "tcgPst15Mn","tcgPst15Sd","tcwPst01Mn","tcwPst01Sd",
               "tcwPst03Mn","tcwPst03Sd","tcwPst07Mn","tcwPst07Sd",
               "tcwPst15Mn","tcwPst15Sd")
x <- data.frame(sapply(x[,variables],as.numeric))
y <- data.frame(y[c("ChangeType")])
# filters by disturbance type 
disturbances <- c("Avalanche","Inter-annual Variability","Clearing",
                  "Mass Movement","Progressive Defoliation","Riparian",
                  "Fire","Post Fire Defoliation","Tree Toppling",
                  "Development","Post Clearing","Post Tree Toppling","Water")
y[y[,1]=="Post Fire Defoliation",] <- "Fire"
# EDIT 'disturbances' TO CHANGE DISTURBANCES INCLUDED IN MODEL
dist_subset <- c(1,2,3,5,6,7,9)
disturbances <- disturbances[dist_subset]
dists <- length(disturbances)
x <- x[y[,1]%in%disturbances,]
y <- y[y[,1]%in%disturbances,,drop=FALSE]
```
#### Train-Test Split
```{r train-test split}
# creates a training and testing set
x_train <- data.frame(matrix(0,0,ncol(x)))
y_train <- data.frame(matrix("",0,1))
x_test <- data.frame(matrix(0,0,ncol(x)))
y_test <- data.frame(matrix("",0,1))
# EDIT 'split' TO CHANGE THE TRAINING SPLIT
split <- 0.75
for (disturbance in disturbances) {
  x_subset <- x[y[,1]==disturbance,]
  y_subset <- y[y[,1]==disturbance,,drop=FALSE]
  num_rows <- nrow(x_subset)
  set.seed(1)
  index_subset <- sample.int(num_rows,num_rows*split)
  x_train <- rbind(x_train,x_subset[index_subset,])
  y_train <- rbind(y_train,y_subset[index_subset,,drop=FALSE])
  x_test <- rbind(x_test,x_subset[-index_subset,])
  y_test <- rbind(y_test,y_subset[-index_subset,,drop=FALSE])
}
y_train <- factor(sapply(y_train,as.factor))
y_test <- factor(sapply(y_test,as.factor))
```
#### Random Forest Training
```{r random forest iterations}
# constructs vector to balance the training data
sample <- table(y_train)
sample <- replace(sample,sample>100,100)
# EDIT 'reps' TO CHANGE THE NUMBER OF TREES IN EACH RANDOM FOREST
reps <- 1000
# runs random forest and extracts most important variables
forest <- randomForest(x=x_train,y=y_train,importance=TRUE,ntree=reps,sampsize=sample)
gini <- forest$importance[,"MeanDecreaseGini"]
# runs random forest using different combinations of variables
# EDIT 'percentages' TO CHANGE PERCENTILE CUTOFFS FOR VARIABLES
percentages <- seq(0,0.9,0.1)
results <- data.frame(matrix(0,length(percentages),9))
colnames(results) <- c("numPredictors","oobError","avgClassError",
                       "maxError","1stPredictor","2ndPredictor",
                       "3rdPredictor","4thPredictor","5thPredictor")
predictions <- data.frame(matrix(0,length(percentages),dists))
colnames(predictions) <- sort(disturbances)
min_class_error <- 99999
best_variables <- NULL
for (i in 1:length(percentages)) {
  x_subset <- x_train[,gini>quantile(gini,probs=percentages[i])]
  forest_subset <- randomForest(x=x_subset,y=y_train,importance=TRUE,ntree=reps,sampsize=sample)
  results[i,1] <- ncol(x_subset)
  results[i,2] <- round(forest_subset$err.rate[reps,"OOB"]*100,2)
  avg_class_error <- round(mean(forest_subset$confusion[,"class.error"])*100,2)
  # stores predictor variables used during the lowest error run
  results[i,3] <- avg_class_error
  if (avg_class_error < min_class_error) {
    min_class_error <- avg_class_error
    best_variables <- colnames(x_subset)
  }
  results[i,4] <- round(max(forest_subset$confusion[,"class.error"])*100,2)
  gini_subset <- data.frame(forest_subset$importance[,"MeanDecreaseGini"])
  colnames(gini_subset) <- c("gini")
  sorted_gini <- rownames(gini_subset)[order(gini_subset$gini,decreasing=TRUE)]
  results[i,c(5,6,7,8,9)] <- sorted_gini[c(1,2,3,4,5)]
  predictions[i,] <- round(forest_subset$confusion[,"class.error"]*100,2)
}
# displays results
sample_print <- as.data.frame(sample)
colnames(sample_print) <- c("disturbance","frequency")
kable(sample_print,align="l",caption="frequency distribution of disturbances used to train model")
kable(results,align="l",row.names=TRUE,caption="error rates and top predictors from all runs")
kable(predictions,align="l",row.names=TRUE,caption="disturbance specific errors from all runs")
```
#### Random Forest Testing
```{r random forest testing}
# runs random forest with most predictive variables and displays results
x_train_subset <- x_train[,best_variables]
x_test_subset <- x_test[,best_variables]
best_forest <- randomForest(x=x_train_subset,y=y_train,xtest=x_test_subset,
                            ytest=y_test,importance=TRUE,ntree=reps,sampsize=sample)
# calculates statistics for random forest run
confusion_train <- best_forest$confusion
confusion_train[,"class.error"] <- round(confusion_train[,"class.error"]*100,2)
colnames(confusion_train)[length(colnames(confusion_train))] <- "Error"
oob_error <- round(best_forest$err.rate[reps,"OOB"]*100,2)
class_error_train <- round(mean(best_forest$confusion[,"class.error"])*100,2)
confusion_test <- best_forest$test$confusion
confusion_test[,"class.error"] <- round(confusion_test[,"class.error"]*100,2)
colnames(confusion_test)[length(colnames(confusion_test))] <- "Error"
class_error_test <- round(mean(best_forest$test$confusion[,"class.error"])*100,2)
# displays results
kable(confusion_train,align="l",caption=paste
      ("confusion matrix for best training run with ",
        ncol(x_train_subset)," variables (avg class error: ",
        class_error_train,", oob_error: ",oob_error,")",sep=""))
kable(confusion_test,align="l",caption=paste
      ("confusion matrix for test set with ",ncol(x_train_subset),
        " variables (avg class error: ",class_error_test,")",sep=""))
```
#### Plotting Test Results
```{r plotting test results,warning=FALSE,fig.align='center'}
# gets the percent of trees that voted for each winning classification
votes <- best_forest$test$votes
vote_percents <- data.frame(matrix(0,nrow(votes),2))
for (i in 1:nrow(votes)) {
  row <- votes[i,]
  vote_percents[i,1] <- max(row)
  vote_percents[i,2] <- colnames(votes)[which.max(row)]
}
# divides these winning classification into bins based on confidence
thresholds_graph <- data.frame(matrix(0,4,dists+1))
thresholds_graph[,8] <- c("80-100%","60-80%","40-60%","20-40%")
for (i in 1:dists) {
  vote_subset <- vote_percents[vote_percents[,2]==disturbances[i],1]
  den <- length(vote_subset)
  thresholds_graph[1,i] <- (length(vote_subset[(vote_subset>=.8&vote_subset<=1)])/den)*100
  thresholds_graph[2,i] <- (length(vote_subset[(vote_subset>=.6&vote_subset<.8)])/den)*100
  thresholds_graph[3,i] <- (length(vote_subset[(vote_subset>=.4&vote_subset<.6)])/den)*100
  thresholds_graph[4,i] <- (length(vote_subset[(vote_subset>=.2&vote_subset<.4)])/den)*100
}
# plots the results
dist_labels <- c("Avalanche","IntAnnVar","Clearing","MassMov","ProgDef",
                 "Riparian","Fire","PostFireDef","TreeTopp","Devel",
                 "PostClear","PostTreeTopp","Water")
labels_subset <- dist_labels[dist_subset]
colnames(thresholds_graph) <- append(labels_subset,"Confidence")
thresholds_graph <- pivot_longer(data=thresholds_graph,cols=labels_subset,
                                 names_to="Disturbance",values_to="Percent")
ggplot(data=thresholds_graph,aes(x=Disturbance,y=Percent,fill=Confidence))+
  geom_bar(stat="identity",color="black",width=0.9,position=position_stack(reverse=TRUE))+
  labs(y="% of Disturbances",title="Random Forest Classification Confidence Levels")+
  guides(fill=guide_legend(reverse=TRUE))+theme(plot.title=element_text(hjust=0.5))
```

