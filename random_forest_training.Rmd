---
title: "Random Forest Training"
output:
  html_document:
    theme: default
    highlight: pygments
    toc: true
    toc_depth: 3
    toc_float: true
editor_options:
  chunk_output_type: console
---
Author: Dan Wexler\
Date: `r Sys.Date()`

This script walks through the process of training a random forest model for
natural landscape disturbance classification. It is divided into sections that
delineate different parts of the process, such as cleaning the input data,
training the model, testing the model, and evaluating the performance of the
model. Below are the inputs to and outputs of this script.\
INPUTS:\
1) .csv file generated by Google Earth Engine where each row represents a
disturbance polygon and each column is the value of a certain predictor
variable for that polygon.\
2) .csv file (with the same number of rows as the first .csv file) with 
disturbance labels for some or all of the polygons. Should also have a few
addition columns, including annualID, yod, and ChangeDesc.\
OUTPUTS:\
1) Trained random forest model that can be used to predict disturbance
labels for polygons.\
2) Table breaking down probability thresholds and omission rates for each
disturbance in the random forest model.\
3) List of variables used in the final random forest model.\

### (1) Setup
This block of code contains variables that the user can modify to fit their
specific needs. This is the only block of code that requires user input,
barring more involved modification of the script. The in-line comments detail
specifically what each variable represents.
```{r setup 1}
# the path to the folder containing this project
folder <- "C:/Users/dwexler/Desktop/R/Landscape-Disturbance-Classification/"
# the name of the file containing the predictor variables
predictors_file <- "predictors.csv"
# the name of the file containing the labels
labels_file <- "labels.csv"
# if TRUE, filters out disturbance patches from the year 1987
drop_1987 <- TRUE
# if TRUE, withholds disturbance patches within elevation mask from training
elevation_mask <- TRUE
# if TRUE, relabels post fire defoliation to fire, post tree toppling to tree 
# toppling, and post clearing to clearing
group_post_with_parent <- TRUE
# if TRUE, relabels water to inter-annual variability
group_water_with_iav <- TRUE
# if TRUE, removes predictors that are highly correlated with other predictors
remove_correlated_vars <- TRUE
# if TRUE, relabels riparian disturbances above 3000ft to mass movements (see
# the 'Mass Movement' appendix to learn why setting this to FALSE makes sense)
riparian_to_mass_movement <- FALSE
# the subset of disturbances to include in this analysis (see the last block of
# code in this section to learn what number corresponds to each disturbance)
disturbance_subset <- c(1,2,4,5,6,10,11,12)
# the percent of the data for each class to be used to training
train_test_split <- 0.7
# the number of trees in the random forest model
num_trees <- 1000
# if TRUE, the testing set of disturbances with ample data will be restricted
# to better reflect the testing set sizes of disturbances with fewer examples
balance_testing_data <- FALSE
# the minimum distance between disturbances in the training set (meters)
min_train_distance <- 200
# the acceptable omission error rate when testing the random forest, used to 
# determine each disturbance's random forest probability threshold
error_cutoff <- 10
```

This block of code contains a function for creating tables that will be used
throughout the script.
```{r setup 2}
# function for creating tables
create_table <- function(table,title,rows) {
  DT::datatable(table,class="cell-border stripe hover",
                rownames=rows,caption=title,
                options=list(pageLength=nrow(table),dom="t",
                             order=list(classes="no-sort")))
}
```

Here we load the R libraries that are used throughout the script. Some or all 
of them may have to be downloaded.
```{r setup 3,message=FALSE,warning=FALSE}
# loads R libraries
library(randomForest)
library(ggplot2)
library(tidyr)
library(ggpattern)
library(caret)
library(dplyr)
library(DT)
```

Now we load the data. Throughout this script, 'x' will refer to the matrix
containing the predictor variables, and 'y' will refer the labels. Each
row in x corresponds to a disturbance patch, and each column is a predictor
variable. y is a vector of labels. The second half of this block creates
a unique id for each entry in y. These unique ids are already present in x
(they were generated in GEE), but we need to generate them in y because x
and y are out of order. We use these unique ids to align the two arrays, so 
the random forest model knows what label corresponds to each patch.
```{r setup 4}
# loads predictor variables and labels
x <- data.frame(read.csv(paste(folder,"input/",predictors_file,sep="")))
y <- data.frame(read.csv(paste(folder,"input/",labels_file,sep="")))
# creates a unique id for each patch that is used to align the x and y arrays
spl <- strsplit(x$patch_name[1],"_")[[1]]
y <- mutate(y,patch_name=paste(spl[1],spl[2],spl[3],spl[4],spl[5],yod,annualID,sep='_'))
x <- arrange(x,patch_name)
y <- arrange(y,patch_name)
```

This block creates two lists. The first is a list of the disturbance types,
which is used to filter the x and y arrays by disturbance. The second
is a list of shortened labels for the disturbance types, which is used for
displaying charts. Both lists are filtered by the disturbances chosen by the 
user in the first code block, using the 'disturbance_subset' variable. The 
numbers corresponding to each disturbance are also shown here. Finally, a
frequency distribution of all labeled disturbances is displayed.
```{r setup 5}
# a list of disturbance types that is used to filter the data by disturbance
disturbances <- c("Avalanche", # (1)
                  "Clearing", # (2)
                  "Development", # (3)
                  "Fire", # (4)
                  "Inter-annual Variability", # (5)
                  "Mass Movement", # (6)
                  "Post Clearing", # (7)
                  "Post Fire Defoliation", # (8)
                  "Post Tree Toppling", # (9)
                  "Progressive Defoliation", # (10)
                  "Riparian", # (11)
                  "Tree Toppling", # (12)
                  "Water") # (13)
# a list of shortened disturbance labels used for displaying charts
disturbance_labels <- c("Aval","Clear","Devel","Fire","IAV",
                        "MM","PosClear","PosFire","PosTopp",
                        "ProgDef","Ripar","Topp","Water")
# filters the disturbance and label lists to include only selected disturbances
disturbances <- disturbances[disturbance_subset]
disturbance_labels <- disturbance_labels[disturbance_subset]
# the number of selected disturbances
num_dists <- length(disturbances)
# displays a frequency distribution of all labeled disturbances
all_disturbances <- as.data.frame(table(y[y$ChangeType!="",]$ChangeType))
colnames(all_disturbances) <- c("Disturbance","Frequency")
create_table(all_disturbances,"Frequency distribution of all disturbances",FALSE)
```

### (2) Filtering and Cleaning Data
Here, we clean and filter the x and y arrays. Some of these cleaning and
filtering processes are automatically performed, while others are only 
performed if the user specifies in the first code block. Some examples of the 
filtering operations include removing unnecessary columns from x, filtering x 
and y by disturbance type, and removing patches within the elevation mask. The 
in-line comments detail each operation with more specificity.
```{r filtering and cleaning data}
# extracts the list of unique patch ids
patch_names <- x$patch_name
# removes non-predictive columns from the 'x' array
x <- subset(x,select=-c(system.index,annualID,index,patch_name,uniqID,.geo))
# removes patches without a labeled change type
x <- x[y$ChangeType!="",]
y <- y[y$ChangeType!="",]
# filters out patches within the elevation mask
mask <- y$ChangeDesc
if (elevation_mask) {
  x <- x[mask!="Fully in mask",]
  y <- y[mask!="Fully in mask",]
}
# converts 'x' data to numeric and changes NA values to 0
x <- data.frame(sapply(x,as.numeric))
x[is.na(x)] <- 0
# filters out patches from the year 1987
if (drop_1987) {
  y <- y[x$yod!=1987,]
  x <- x[x$yod!=1987,]
}
# extracts the disturbance labels from 'y'
y <- data.frame(y$ChangeType)
# relabels certain disturbances with their parent disturbance
if (group_post_with_parent) {
  y[y[,1]=="Post Fire Defoliation",] <- "Fire"
  y[y[,1]=="Post Tree Toppling",] <- "Tree Toppling"
  y[y[,1]=="Post Clearing",] <- "Clearing"
}
# relabels water disturbances with inter-annual variability
if (group_water_with_iav) {
   y[y[,1]=="Water",] <- "Inter-annual Variability"
}
# relabels high-elevation riparian disturbances with mass movement
mm_tracker <- data.frame(matrix(FALSE,nrow(y),1))
mm_tracker[y[,1]=="Mass Movement",] <- TRUE
mm_text <- "it was not"
if (riparian_to_mass_movement) {
  y[which(y[,1]=="Riparian"&x$elev_mean>914.4),] <- "Mass Movement"
  mm_text <- "it was"
}
# filters data by disturbance type
mm_tracker <- mm_tracker[y[,1]%in%disturbances,,drop=FALSE]
x <- x[y[,1]%in%disturbances,]
y <- y[y[,1]%in%disturbances,,drop=FALSE]
# removes predictors with more than 90% correlation with other variables
if (remove_correlated_vars) {
  correlation <- cor(x,method="spearman")
  correlations <- findCorrelation(cor(x),cutoff=0.9)
  x <- x[,-correlations]
}
```

### (3) Train-Test Split
An important component of many supervised machine learning models is a 
train-test split. The purpose of a train-test-split is to purposefully withhold
some data from the training process, so a model can be tested on unseen data 
later. It's relatively easy to build a machine learning model that 
performs well on data it has already seen before, but it's harder to build one 
that does well on unseen data. This section divides our data into a training 
set and a testing set.\
\
There are a few nuances to our specific case. The first is that, since
some disturbance types are much more frequent than others, we cannot
simply take a random sample of, say, 75% of all of our data and call this the
training set. It is very likely that some disturbances would not be represented
in either the training or testing set. To fix this issue, we perform stratified
sampling, which means we take a random subset of each disturbance type. This
ensures that each disturbance type will be represented. However, we would still
be dealing with extremely imbalanced data, as some disturbances have many more 
examples than other. These discrepancies would propagate into
the training and testing sets, and we don't want our model to only
perform well on the disturbances with the most examples. To fix this, we set
an upper limit on the number of examples of each disturbance that the model
can be trained on. This limit is arbitrarily chosen to be five times the
number of examples present in the least-frequent disturbance class. Another
nuance is that the user is able to specify, using the 'balance_testing_data'
variable, whether the testing data is balanced like the training data. We will
always train the random forest model with a balanced data set, but it is up to
the user whether to test the model on a balanced data set. One advantage of
not balancing the testing set is that, for disturbance classes with a large
number of examples, we get a more representative classification accuracy.
However, an unbalanced testing set will lead to less meaningful comission error
rates. These are important considerations for assessing the model, but its is 
important to remember that balancing or not balancing the testing set will not
affect the inherent performance of a specific random forest model.\
\
The final note is that, to reduce spatial autocorrelation, the training set is
constructed with a spatially diverse set of samples. In this run of the model,
this means that no two disturbances of the same type in the training set
are closer than `r min_train_distance ` meters. This first block of code 
contains a function for calculating the distance between two disturbances.
```{r train-test split 1}
# a function that calculates the Euclidean distance between two disturbances
euclidean_distance <- function(dist1, dist2) {
  # converts from degrees to radians
  lat1 <- dist1$latitude*(pi/180)
  lon1 <- dist1$longitude*(pi/180)
  lat2 <- dist2$latitude*(pi/180)
  lon2 <- dist2$longitude*(pi/180)
  # the radius of the Earth
  R <- 6378100 
  # converts from spherical to Cartesian coordinates
  x1 <- R*cos(lat1)*cos(lon1)
  y1 <- R*cos(lat1)*sin(lon1)
  z1 <- R*sin(lat1)
  x2 <- R*cos(lat2)*cos(lon2)
  y2 <- R*cos(lat2)*sin(lon2)
  z2 <- R*sin(lat2)
  # calculates the Euclidean distance
  return(sqrt((x2-x1)^2+(y2-y1)^2+(z2-z1)^2))
}
```

This code contains the algorithm for building the training and testing sets.
```{r train-test split 2}
# creates empty data frames that will store the train and test sets
x_train <- data.frame()
y_train <- data.frame()
x_test <- data.frame()
y_test <- data.frame()
mm_train <- data.frame()
mm_test <- data.frame()
# the max number of disturbances to include in the train and test sets
max_train_sample <- round(min(table(y))*train_test_split,0)*5
max_test_sample <- (min(table(y))-round(min(table(y))*train_test_split,0))*5
# for each disturbance type
for (disturbance in disturbances) {
  # filter the predictors and labels by disturbance type
  x_subset <- x[y[,1]==disturbance,]
  y_subset <- y[y[,1]==disturbance,,drop=FALSE]
  mm_tracker_subset <- mm_tracker[y[,1]==disturbance,,drop=FALSE]
  # finds the number of the current disturbance
  len_subset <- nrow(x_subset)
  # a vector for storing indices
  train_inds <- c()
  # a variable that will determine the size of the training set
  train_limit <- 0
  # if there are more disturbances than the allowed max
  if (len_subset*train_test_split >= max_train_sample) {
    # set the limit to the max
    train_limit <- max_train_sample
  } else {
    # set the limit to the number of disturbances * the train-test split
    train_limit <- round(len_subset*train_test_split,0)
  }
  # a variable that will keep track of iterations
  counter <- 0
  # while the training set is not full
  while (length(train_inds) < train_limit & counter < len_subset) {
    # pick a random index
    ind <- sample(setdiff(1:len_subset,train_inds),1)
    # if the train subset is not empty
    flag <- TRUE
    if (length(train_inds) > 0) {
      # for every disturbance in the train subset
      for (j in 1:length(train_inds)) {
        # if two disturbances are within a certain distance of each other
        if (euclidean_distance(x_subset[ind,],x_subset[train_inds[j],])<min_train_distance) {
          # change the flag
          flag <- FALSE
        }
      }
    }
    # if the flag is still TRUE
    if (flag) {
      # add the index to the training indices
      train_inds <- append(train_inds,ind)
    }
    # increment the counter
    counter <- counter+1
  }
  # add disturbances to the train set
  x_train <- rbind(x_train,x_subset[train_inds,])
  y_train <- rbind(y_train,y_subset[train_inds,,drop=FALSE])
  mm_train <- rbind(mm_train,mm_tracker_subset[train_inds,,drop=FALSE])
  # a variable that will determine the size of the test set
  test_limit <- 0
  # if there are more remaining disturbances than the allowed max and
  # the 'balance_testing_data' variable is TRUE
  if (length(setdiff(1:len_subset,train_inds)) >= max_test_sample & balance_testing_data) {
    # set the limit to the max
    test_limit <- max_test_sample
  } else {
    # set the limit to the number of remaining disturbances
    test_limit <- length(setdiff(1:len_subset,train_inds))
  }
  # generate a random sample of indices
  test_inds <- sample(setdiff(1:len_subset,train_inds),test_limit,replace=FALSE)
  # add disturbances to the test set
  x_test <- rbind(x_test,x_subset[test_inds,])
  y_test <- rbind(y_test,y_subset[test_inds,,drop=FALSE])
  mm_test <- rbind(mm_test,mm_tracker_subset[test_inds,,drop=FALSE])
}
# converts the labels to factor type for random forest modeling
y_train <- factor(sapply(y_train,as.factor))
y_test <- factor(sapply(y_test,as.factor))
```

### (4) Random Forest Training
We are now ready to train a random forest model. This block of code trains
an initial model using all of the available predictor variables and extracts
an ordered list of the most important. This importance metric for a given 
variable is an average of two quantities generated by the random forest 
algorithm: the mean decrease in gini and the mean decrease in accuracy. The
mean decrease in gini measures how the entropy, or variance, of a branch of
a tree decreases by splitting on a certain variable. The mean decrease in
accuracy measures the loss in model accuracy that would occur if a specific
predictor was removed from the model. The second block of code uses this 
importance ranking to train random forest models with successively fewer and
fewer predictor variables. The best model is found among these iterations.
```{r random forest training 1,fig.align='center'}
# trains a random forest model
forest <- randomForest(x=x_train,y=y_train,importance=TRUE,ntree=num_trees)
# creates importance metric from gini and accuracy values
gini <- forest$importance[,"MeanDecreaseGini"]
accuracy <- forest$importance[,"MeanDecreaseAccuracy"]
importance <- sort((gini+accuracy)/2,decreasing=TRUE)
# displays plots of predictor variables ranked by gini and accuracy values
varImpPlot(forest,sort=TRUE,n.var=15,main="Random Forest Variable Importance")
```

Now that we have a list of the most important variables, we train several
random forest models, dropping a number of the least predictive variables
each iteration. The random forest with the lowest average class error is saved.
Average class error is the average of each disturbance's classification, or
omission, error. There are other metrics that could be used to decide which
model is the best, such as the average comission error or the total error.
The average class error was chosen because we want this model to perform well
on all classes, not just those with more examples. The average class error
for each iteration is shown in a chart.
```{r random forest training 2}
# a variable that will be used to compare successive random forest models
min_class_err <- Inf
# a variable that will store the best set of variables
best_vars <- NULL
# a variable that will store the best random forest
best_forest <- NULL
# the total number of predictor variables
num_vars <- ncol(x_train)
# how many predictors to drop each time a new random forest model is created
drop <- round(num_vars*0.1,0)
# creates a data frame to store the average class errors from each run
results <- data.frame()
# a variable to track where to store values in the 'results' array
row <- 1
# while there are variables left to create a random forest model with
while (num_vars > 1) {
  # stores the number of predictors used in this iteration
  results[row,1] <- num_vars
  # gets the names of the predictors used in this iteration
  vars <- names(importance[1:num_vars])
  # filters the train and test arrays by the predictors used in this iteration
  x_train_subset <- x_train[,vars]
  x_test_subset <- x_test[,vars]
  # trains a random forest model with a subset of predictor variables
  forest <- randomForest(x=x_train_subset,y=y_train,xtest=x_test_subset,
                         ytest=y_test,ntree=num_trees,keep.forest=TRUE)
  # calculates the average class error of the random forest model
  avg_class_err <- mean(forest$confusion[,"class.error"])
  # stores the average class error
  results[row,2] <- round(avg_class_err*100,2)
  # if this is the run with the lowest average class error, store the
  # number of variables and save the random forest
  if (avg_class_err < min_class_err) {
    min_class_err <- avg_class_err
    best_vars <- vars
    best_forest <- forest
  }
  # drop a certain number of predictors for the next iteration
  num_vars <- num_vars-drop
  row <- row+1
}
# converts the best variables to a data frame type
best_vars <- data.frame(best_vars)
# displays a chart showing the average class error for each random forest run
colnames(results) <- c("Number of Predictors","Average Class Omission Error (%)")
create_table(results,"Average class omission error for each random forest",FALSE)
```

### (5) Random Forest Testing {.tabset .tabset-pills}
We have found our best random forest model (with the top `r nrow(best_vars)` 
predictors), so now let us look at the results. We can use omission and 
comission error rates, as well as the F1 score, to assess the accuracy of our 
model. The following chunk of code creates confusion matrices for the training 
and testing sets. Click the 'Training' and 'Testing' buttons below to toggle 
between statistics for the training and testing data sets.
```{r random forest testing 1}
# a function that creates and formats a confusion matrix for display
create_confusion_matrix <- function(confusion) {
  # rounds omission error to two decimal points
  confusion[,"class.error"] <- round(confusion[,"class.error"]*100,2)
  # changes a column name
  colnames(confusion) <- append(disturbance_labels,"Omission Error (%)")
  # calculates the comission error for each disturbance type
  comissions <- list()
  for (i in 1:num_dists) {
    comission <- round((1-(confusion[i,i]/sum(confusion[,i])))*100,2)
    comissions <- append(comissions,comission)
  }
  # adds the comission error to the confusion matrix
  confusion <- rbind(confusion,append(comissions,NA))
  rownames(confusion) <- append(disturbance_labels,"Comission Error (%)")
  return (confusion)
}
# calls the function above and calculates average errors for the training set 
confusion_train <- create_confusion_matrix(best_forest$confusion)
omission_train <- round(mean(best_forest$confusion[,"class.error"])*100,2)
comission_train <- round(mean(as.numeric(confusion_train["Comission Error (%)",1:num_dists])),2)
# calls the function above and calculates average errors for the testing set 
confusion_test <- create_confusion_matrix(best_forest$test$confusion)
omission_test <- round(mean(best_forest$test$confusion[,"class.error"])*100,2)
comission_test <- round(mean(as.numeric(confusion_test["Comission Error (%)",1:num_dists])),2)
```

This chunk of code calculates the F1 score for each disturbance type in the
training and testing data sets. The F1 score is a combined measure of the 
recall and precision, which are the complements of the omission and comission
error rates, respectively. This code also formats tables for display.
```{r random forest testing 2}
# a function that creates and formats a table displaying F1 scores
create_f1_table <- function(confusion) {
  # calculates the recall, the complement of omission error
  recall <- 100-as.numeric(confusion[1:nrow(confusion)-1,ncol(confusion)])
  # calculates the precision, the complement of comission error
  precision <- 100-as.numeric(confusion[nrow(confusion),1:ncol(confusion)-1])
  # calculates the F1 score using precision and recall
  f1 <- round((2*(recall*precision)/(recall+precision))/100,2)
  # formats the table
  f1_table <- data.frame(matrix(0,num_dists,2))
  f1_table[,1] <- disturbances
  f1_table[,2] <- f1
  colnames(f1_table) <- c("Disturbance","F1 Score")
  return (f1_table)
}
# calls the function above and calculates the mean F1 score for the training set
f1_train <- create_f1_table(confusion_train)
f1_train_mean <- round(mean(f1_train[,2]),2)
# calls the function above and calculates the mean F1 score for the testing set
f1_test <- create_f1_table(confusion_test)
f1_test_mean <- round(mean(f1_test[,2]),2)
```

#### Training Results
This code displays the frequency distribution of the training set.
```{r random forest testing 3}
# displays a frequency distribution of the training set
train_table <- as.data.frame(table(y_train))
colnames(train_table) <- c("Disturbance","Frequency")
create_table(train_table,"Frequency distribution of the training set",FALSE)
```

This code displays a confusion matrix for the training set. The rows represent 
real disturbance labels, and the columns represent the random forest's labels.
```{r random forest testing 4}
# displays a confusion matrix for the training set
caption <- paste("Training set confusion matrix (average omission error: ",
                 omission_train,"% | average comission error: ",comission_train,"%)",sep="")
create_table(confusion_train,caption,TRUE)
```

This code displays a table of F1 scores for disturbances in the training set.
```{r random forest testing 5}
# displays the F1 scores for the training set
caption <- paste("F1 scores for the training set (average F1 score: ",f1_train_mean,")",sep="")
create_table(f1_train,caption,FALSE)
```

#### Testing Results
This code displays the frequency distribution of the testing set.
```{r random forest testing 6}
# displays a frequency distribution for the testing set
test_table <- as.data.frame(table(y_test))
colnames(test_table) <- c("Disturbance","Frequency")
create_table(test_table,"Frequency distribution of the testing set",FALSE)
```

This code displays a confusion matrix for the testing set. The rows represent 
real disturbance labels, and the columns represent the random forest's labels.
```{r random forest testing 7}
# displays a confusion matrix for the testing set
caption <- paste("Testing set confusion matrix (average omission error: ",
                 omission_test,"% | average comission error: ",comission_test,"%)",sep="")
create_table(confusion_test,caption,TRUE)
```

This code displays a table of F1 scores for disturbances in the testing set.
```{r random forest testing 8}
# displays the F1 scores for the testing set
caption <- paste("F1 scores for the testing set (average F1 score: ",f1_test_mean,")",sep="")
create_table(f1_test,caption,FALSE)
```

### (6) Probability Evaluation
When our random forest makes a classification, it is taking the majority vote
among its `r num_trees` trees. The percent of trees that vote for the winning
class is the probability that a patch belongs to that disturbance class,
according to the model. For example, if `r num_trees/2` out of `r num_trees`
trees vote for a certain disturbance class, the model is assigning that patch
a 50% probability of being that certain disturbance. The code below evaluates,
for each disturbance type individually, how the omission error over the testing
data changes as we threshold by this probability. Does the omission error rate
decrease as we look only at classifications made with higher and higher
probabilities? If that is the case, we can imagine setting an omission error
cutoff (the classification error we are comfortable with), and finding the
probability threshold for each disturbance at which the omission error is at or
below the error cutoff we set.
```{r probability evaluation 1}
# converts the testing labels to a suitable data structure for analysis
y_test <- as.data.frame(y_test)
# creates a list of probability thresholds
confidences <- seq(0,1,0.05)
# extracts the forest's voting distribution over the testing set
votes <- best_forest$test$votes
# extracts the forest's predictions for each disturbance patch
predicted <- as.data.frame(best_forest$test$predicted)
# creates a data frame to store error results
results <- data.frame(matrix(0,length(confidences),num_dists+1))
results[,1] <- round(confidences*100,0)
# creates a data frame to store probability threshold results
cutoffs <- data.frame(matrix(NA,num_dists,5))
# creates a logical vector that will be used to help store data
flags <- vector("logical",num_dists)
# for each probability threshold
for (i in 1:length(confidences)) {
  confidence <- confidences[i]
  # for each disturbance
  for (j in 1:num_dists) {
    # filters the voting array by the current disturbance
    votes_subset <- votes[y_test==disturbances[j],]
    # filters the label array by the current disturbance
    y_test_subset <- y_test[y_test==disturbances[j],,drop=FALSE]
    # filters the prediction array by the current disturbance
    predicted_subset <- predicted[y_test==disturbances[j],,drop=FALSE]
    # numerator and denominator used to calculate error
    num <- 0
    den <- 0
    # for each classified disturbance
    for (k in 1:nrow(votes_subset)) {
      # if the classification was made above a certain probability
      if (max(votes_subset[k,])>=confidence) {
        # increment the denominator
        den <- den+1
        # if the classification was correct
        if (predicted_subset[k,1]==y_test_subset[k,1]) {
          # increment the numerator
          num <- num+1
        }
      }
    }
    # calculate the disturbance's omission error at the current probability
    error <- round((1-(num/den))*100,2)
    # store the error result
    results[i,j+1] <- error
    # if error is not null (if any classifications were made)
    if (!is.na(error)) {
      # if error is below the cutoff and a value hasn't been stored
      if (error <= error_cutoff & !flags[j]) {
        # store the probability threshold
        cutoffs[j,1] <- round(confidence*100,0)
        # calculate the percent of the current disturbances classified
        percent_classified <- round(den/test_table$Frequency[j]*100,2)
        # store the percent of current disturbances classified
        cutoffs[j,2] <- percent_classified
        # store the number of current disturbances classified
        cutoffs[j,3] <- den
        # stores the omission error
        cutoffs[j,4] <- error
        sig <- "no"
        if (den>=20) {
          sig <- "yes"
        }
        cutoffs[j,5] <-sig
        # flip the disturbance's flag so no more data will be stored
        flags[j] <- TRUE
      }
    }
  }
}
# calculates the mean omission error
mean_error <- round(mean(cutoffs[,4],na.rm=TRUE),2)
```

This chart shows the omission error for each disturbance class broken down by 
probability threshold. Blank spaces indicate that no classifications for a 
certain class were made above a certain probability threshold, and thus there 
is no error rate. In percent.
```{r probability evaluation 2}
# displays the chart described above
colnames(results) <- append("Prob",disturbance_labels)
caption <- "Omission error for each disturbance at different probability thresholds"
create_table(results,caption,FALSE)
```

This chart shows the probability threshold at which
each disturbance achieves an omission error rate at or below the user chosen
value of `r error_cutoff`%. It also displays, at that probability threshold,
what percent of a disturbance's testing set was classified. Blank spaces
indicate that a disturbance never achieved a suitable error rate.
```{r confidence evaluation 3}
# displays the chart described above
rownames(cutoffs) <- disturbance_labels
colnames(cutoffs) <- c("Probability (%)","Classified (%)",
                       "Num Classified","Omission Error (%)","Signifigant")
caption <- paste("What probability threshold causes omission error to be below ",
                 error_cutoff,"%",sep="")
create_table(cutoffs,caption,TRUE)
```

### (7) Saving the Model
This block of code saves the trained random forest as well as two relevant
supplemental files. The first supplemental file contains the probability
threshold and associated omission error rate for each disturbance, calculated
in the section above. The second supplemental file is the list of variables
used in the final random forest model. These two files, along with the model
itself, will allow the user to apply the model to unseen disturbance patches.
These files will save to the folder in which this script is located.
```{r saving the model}
# sets the path to which files will be saved
intermediate <- paste(folder,"intermediate/","best_random_forest",sep="")
# saves the probability threshold and omission error rate for each disturbance
metrics <- cutoffs[,c(1,4,5)]
rownames(metrics) <- disturbances
colnames(metrics) <- c("probablility","error","signifigant")
write.csv(metrics,file=paste(intermediate,"_metrics.csv",sep=""))
# saves the variables used in the random forest model
write.csv(best_vars,file=paste(intermediate,"_variables.csv",sep=""),row.names=FALSE)
# saves the trained random forest model
saveRDS(best_forest,file=paste(intermediate,".RData",sep=""))
```

### (8) Appendix {.tabset .tabset-pills}
#### Mass Movements
NOTE: If the 'riparian_to_mass_movement' variable was set to TRUE in the 
first code block (`r mm_text`), proceed. If not, set this variable to TRUE,
rerun this script, and then return to this section.\

Riparian disturbances above a certain elevation may be interpreted as mass
movements. Should we set an elevation cutoff and relabel all riparian patches
above that cutoff as mass movements? This block of code examines whether this
makes sense by comparing the omission error of the original mass movement
patches to the omission error of the relabeled patches, in both the training
and testing sets. For this change to make sense, the error rates should be
close together. If the error rates for changed mass movements are lower than
those for the original mass movements, we can conclude that the changed mass
movements are not helping to better describe the data space of the original 
mass movements to the random forest model.
```{r mass movements 1}
# a function that calculates mass movement error rates
mm_error <- function(labels,predicted,tracker) {
  # numerators used to calculate error
  orig_num <- 0
  chng_num <- 0
  # for each predicted disturbance
  for (i in 1:nrow(predicted)) {
    # if the prediction was a mass movement
    if (predicted[i,1]=="Mass Movement") {
      # if the patch is an original mass movement
      if (tracker[i,1]==TRUE) {
        # iterate the numerator for original mass movements
        orig_num <- orig_num+1
      # if the patch is a changed mass movement
      } else if (tracker[i,1]==FALSE & labels[i,1]=="Mass Movement") {
        # iterate the numerator for changed mass movements
        chng_num <- chng_num+1
      }
    }
  }
  # calculates the errors for original, changed, and combined mass movements
  num_orig <- sum(tracker[,1]==TRUE)
  num_comb <- sum(labels[,1]=="Mass Movement")
  orig_error <- round((1-(orig_num/num_orig))*100,2)
  chng_error <- round((1-(chng_num/(num_comb-num_orig)))*100,2)
  comb_error <- round((1-((orig_num+chng_num)/num_comb))*100,2)
  errors <- c(orig_error,chng_error,comb_error)
  return(errors)
}
# calls the function above for the training set
y_train <- as.data.frame(y_train)
train_predicted <- as.data.frame(best_forest$predicted)
train_errors <- mm_error(y_train,train_predicted,mm_train)
# calls the function above for the testing set
y_test <- as.data.frame(y_test)
test_predicted <- as.data.frame(best_forest$test$predicted)
test_errors <- mm_error(y_test,test_predicted,mm_test)
```

This chart shows the error rates for the original and changed mass movements
for both the training and the testing set.
```{r mass movements 2}
# displays the chart described above
errors_chart <- data.frame(matrix(0,3,2))
errors_chart[,1] <- train_errors
errors_chart[,2] <- test_errors
rownames(errors_chart) <- c("Original","Changed","All")
colnames(errors_chart) <- c("Train Error (%)","Test Error (%)")
caption <- "Omission error rates for original and modified mass movements"
create_table(errors_chart,caption,TRUE)
```

#### Correlation
Here we calculate the Spearman Correlation between the size of each 
disturbance's training set and respective testing performance.
```{r correlation}
# calculates the Spearman Correlation between training size and testing error
train_examples <- (table(y_train))
omission_error <- 1-best_forest$test$confusion[,"class.error"]
spearman <- round(cor(train_examples,omission_error,method="spearman"),2)
```
The Spearman Correlation between the size of each disturbance's training set and
respective testing performance is: `r spearman`\
\

#### Plotting Test Results
This section includes a few plots that will help us analyze our random forest
model. The first plot displays, for each disturbance type, at what confidence 
level correct classifications were made. Confidence will refer to the percent 
of trees in our forest that vote for a specific patch being a certain 
disturbance type. For example, if 80% of the trees in the forest vote that a
specific patch is an avalanche, we will say our forest is 80% confident that 
said patch is an avalanche. This first plot shows the distribution of these
confidence levels for correct classifications made by the model. All plots in
this section are analyzing the testing data, not the training.
```{r plotting test results 1,warning=FALSE,fig.align='center'}
# y_test <- factor(sapply(y_test,as.factor))
# # gets the distribution of random forest votes for every disturbance
# votes <- best_forest$test$votes
# # creates a data frame to store results
# vote_percents <- data.frame()
# # for every disturbance patch
# for (i in 1:nrow(votes)) {
#   # finds the disturbance type that received the most votes
#   label <- colnames(votes)[which.max(votes[i,])]
#   # if a correct classification was made
#   if (label==y_test[i]) {
#     # store the disturbance type and the percent of votes that type received
#     row_num <- nrow(vote_percents)+1
#     vote_percents[row_num,1] <- max(votes[i,])
#     vote_percents[row_num,2] <- label
#   }
# }
# # creates a data frame to store results
# thresholds_graph <- data.frame(matrix(0,4,num_dists+1))
# # sets the last column of the data frame equal to percentage bins
# thresholds_graph[,num_dists+1] <- c("80-100%","60-80%","40-60%","0-40%")
# # for each disturbance type
# for (i in 1:num_dists) {
#   # filters the 'vote_percents' array by disturbance type
#   subset <- vote_percents[vote_percents[,2]==disturbances[i],1]
#   # finds the number of disturbances after filtering
#   len <- length(subset)
#   # divides the correct classifications into bins based on confidence
#   thresholds_graph[1,i] <- (length(subset[(subset>=.8&subset<=1)])/len)*100
#   thresholds_graph[2,i] <- (length(subset[(subset>=.6&subset<.8)])/len)*100
#   thresholds_graph[3,i] <- (length(subset[(subset>=.4&subset<.6)])/len)*100
#   thresholds_graph[4,i] <- (length(subset[(subset>=0&subset<.4)])/len)*100
# }
# # sets the column names of the 'thresholds_graph' array
# colnames(thresholds_graph) <- append(disturbance_labels,"Confidence")
# # reorganizes array be suitable for plotting, pivots from wide to long
# thresholds_graph <- pivot_longer(data=thresholds_graph,
#                                  cols=all_of(disturbance_labels),
#                                  names_to="Disturbance",
#                                  values_to="Percent")
# # plots the results
# ggplot(data=thresholds_graph,
#        aes(x=Disturbance,y=Percent,fill=Confidence),pattern="stripe")+
#   geom_bar_pattern(position=position_stack(reverse=TRUE),stat="identity",
#                    width=0.8,color="black",aes(pattern_angle=Confidence),
#                    pattern_fill="white",pattern_color="white",
#                    pattern_spacing=.02,pattern_density=0.05)+
#   scale_pattern_angle_manual(values=c(0,45,90,-45))+
#   labs(x="Disturbance Type",y="% of Disturbance Classifications",
#        title="Random Forest Correct Classification Confidence Levels")+
#   theme(plot.title=element_text(hjust=0.5))
```

This second plot shows, for each disturbance, how the classification (or
omission) error changes as we threshold by confidence level. The plot above
shows us the distribution of these confidence levels for correct
classifications, and this plot tells us if confidence levels matter. For
example, if we find that the classification errors for most classes drop
significantly if we only look at disturbance patches classified with above
50% confidence, we may choose to only accept results from the random forest
model that were classified above this threshold. It is also easy to imagine
setting an individual threshold for each disturbance type, depending on how
the classification error changes as we adjust the threshold.
```{r plotting test results 2,warning=FALSE,fig.align='center'}
# # gets the classification error of each class with a confidence threshold
# y_test <- as.data.frame(y_test)
# thresholds <- c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)
# threshold_data <- data.frame(matrix(0,length(thresholds),num_dists+1))
# threshold_data[,num_dists+1] <- thresholds*100
# # loops through the thresholds
# for (i in 1:length(thresholds)) {
#   threshold <- thresholds[i]
#   # loops through the disturbance types
#   for (j in 1:num_dists) {
#     vote_subset <- votes[y_test==disturbances[j],]
#     predicted_subset <- best_forest$test$predicted[y_test==disturbances[j]]
#     y_test_subset <- y_test[y_test==disturbances[j],,drop=FALSE]
#     num <- 0
#     den <- 0
#     # finds the percent of a disturbance type correctly classified with a
#     # confidence above the current threshold
#     for (k in 1:nrow(vote_subset)) {
#       if (max(vote_subset[k,]) >= threshold) {
#         den <- den+1
#         if (predicted_subset[k]==y_test_subset[k,1]) {
#           num <- num+1
#         }
#       }
#     }
#     threshold_data[i,j] <- round((1-(num/den))*100,2)
#   }
# }
# # plots the results
# colnames(threshold_data) <- append(disturbance_labels,"Threshold")
# threshold_data <- pivot_longer(data=threshold_data,cols=all_of(disturbance_labels),
#                                  names_to="Disturbance",values_to="Percent")
# points <- c(0,1,2,5,15,16,17,18,3,4,12,13,8)
# point_subset <- points[1:num_dists]
# ggplot(data=threshold_data,aes(x=Threshold,y=Percent,group=Disturbance))+
#   geom_line(aes(color=Disturbance),linewidth=1)+
#   geom_point(aes(shape=Disturbance,color=Disturbance),size=3)+
#   scale_shape_manual(values=c(0,1,2,5,15,16,17,18))+
#   xlim(0,100)+ylim(0,100)+theme(plot.title=element_text(hjust=0.5))+
#   labs(x="Confidence Threshold (%)",y="Class Error (%)",
#        title="RF Classification Error by Confidence")
```

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
