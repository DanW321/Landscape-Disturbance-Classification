---
title: "Random Forest Training"
output:
  html_document:
    theme: default
    highlight: pygments
    toc: true
    toc_depth: 4
    toc_float: true
editor_options:
  chunk_output_type: console
---
Author: Dan Wexler\
Date: `r Sys.Date()`
```{r start,include=FALSE}
# file settings
knitr::opts_chunk$set(include=TRUE,echo=TRUE)
```

This script walks through the process of training a random forest model for
natural landscape disturbance classification. Below are the inputs to and
outputs of this script.\
INPUTS:\
1) .csv file generated by Google Earth Engine where each represents a
disturbance polygon and each column is the value of a certain predictor
variable for that polygon.\
2) .csv file with the same number of rows as the first .csv file with 
disturbance labels for some or all of the polygons. Should also have a few
addition columns, including annualID, yod, and ChangeDesc.\
OUTPUTS:\
1) Trained random forest model that can be used for predicting disturbance
labels for polygons\
2) Table breaking down confidence thresholds for the random forest model.\
\

#### (1) Setup
This block of code contains variables that the user should modify to fit their
specific needs. Barring more involved modification of the script, this is the
only block of code that requires user input. The in-line comments detail
exactly what each variable represents.
```{r setup 1}
# the path to the folder containing the predictor variables and labels
folder <- "C:/Users/dwexler/Desktop/R/nps-repo/rf/"
# the name of the predictor variables file
data_file_name <- "predictor_vars.csv"
# the name of the labels file
label_file_name <- "rf_labels.csv"
# if TRUE, withholds disturbance patches within elevation mask from training
elevation_mask <- TRUE
# if TRUE, relabels post fire defoliation to fire, post tree toppling to tree 
# toppling, and post clearing to clearing
group_post_with_parent <- TRUE
# if TRUE, relabels water to inter-annual variability
group_water_with_iav <- TRUE
# if TRUE, removes predictors that are highly correlated with other predictors
remove_correlated_vars <- FALSE
# if TRUE, relabels riparian disturbances above 3000ft to mass movements
riparian_to_mass_movement <- FALSE
# the subset of disturbances to include in this analysis, look at last block
# in this section to see what number corresponds to each disturbance
disturbance_subset <- c(1,2,4,5,6,10,11,12)
# the percent of the data for each class to be used to training
train_test_split <- 0.75
# the number of trees to construct when building a random forest
num_trees <- 5000
# if TRUE, the testing set of disturbances with ample data will be restricted
# to better reflect the testing set sizes of disturbances with fewer examples
balance_testing_data <- TRUE
# when testing the random forest, the acceptable comission error rate for each
# class, used to determine each disturbance's appropriate confidence threshold
error_cutoff <- 5
```

Here we load libraries that are used throughout the script. You may have to
download some or all of them.
```{r setup 2,message=FALSE,warning=FALSE}
library(randomForest)
library(knitr)
library(ggplot2)
library(tidyr)
library(ggpattern)
library(caret)
library(shiny)
library(dplyr)
library(DT)
```

Now we load the data. Throughout this script, 'x' will represent the matrix
containing the predictor variables, and 'y' will represent the labels. Each
row in 'x' corresponds to a disturbance patch, and each column is a predictor
variable. 'y' is a vector of labels. The second half of this block creates
a unique id for each entry in 'y'. These unique ids are already present in 'x'
(they were generated in GEE), but we need to generate them in 'y' because 'x'
and 'y' are out of order. We use these unique ids to align the two arrays, so 
the random forest model knows what label corresponds to what data.
```{r setup 3}
# loads predictor variables and labels
x <- data.frame(read.csv(paste(folder,data_file_name,sep="")))
y <- data.frame(read.csv(paste(folder,label_file_name,sep="")))
# creates a unique id for each patch that is used to align the x and y arrays
spl <- strsplit(x$patch_name[1],"_")[[1]]
y <- mutate(y,patch_name=paste(spl[1],spl[2],spl[3],spl[4],spl[5],yod,annualID,sep='_'))
x <- arrange(x,patch_name)
y <- arrange(y,patch_name)
```

This block creates two lists. The first is a list of the disturbance types,
which is used to filter the 'x' and 'y' arrays by disturbance. The second
is a list of shortened labels for the disturbance types, which is used for
plotting. Both lists are filtered by the disturbances chosen by the user, and 
the numbers corresponding to each disturbance are shown here. Finally, a
frequency distribution of all labeled disturbances is displayed.
```{r setup 4}
# a list of disturbance types that is used to filter the data by disturbance
disturbances <- c("Avalanche", # (1)
                  "Clearing", # (2)
                  "Development", # (3)
                  "Fire", # (4)
                  "Inter-annual Variability", # (5)
                  "Mass Movement", # (6)
                  "Post Clearing", # (7)
                  "Post Fire Defoliation", # (8)
                  "Post Tree Toppling", # (9)
                  "Progressive Defoliation", # (10)
                  "Riparian", # (11)
                  "Tree Toppling", # (12)
                  "Water") # (13)
# a list of shortened disturbance labels used for plotting
disturbance_labels <- c("Aval","Clear","Devel","Fire","IAV",
                        "MassMov","PosClear","PosFireDef","PosTreeTopp",
                        "ProgDef","Riparian","TreeTopp","Water")
# filters the disturbance and label lists to include only selected disturbances
disturbances <- disturbances[disturbance_subset]
disturbance_labels <- disturbance_labels[disturbance_subset]
# the number of selected disturbances, used throughout the script
num_dists <- length(disturbances)
# displays a frequency distribution of all labeled disturbances
all_disturbances <- as.data.frame(table(y[y$ChangeType!="",]$ChangeType))
colnames(all_disturbances) <- c("Disturbance","Frequency")
DT::datatable(all_disturbances,
              class='cell-border stripe hover',
              rownames=FALSE,
              options=list(pageLength=nrow(all_disturbances),dom="t"),
              caption="Frequency distribution of all disturbances")
```

#### (2) Filtering and Cleaning Data
Here, we clean and filter the 'x' and 'y' arrays. Some of these cleaning and
filtering processes are automatically performed, while others are at the
discretion of the user. Some examples of the filtering operations include
removing unnecessary columns from 'x', filtering 'x' and 'y' by disturbance 
label, and filtering out patches within the elevation mask. The in-line 
comments detail each operation with more specificity.
```{r filtering and cleaning data}
# extracts the list of unique patch ids
patch_names <- x$patch_name
# removes non-predictive columns from the 'x' array
x <- subset(x,select=-c(system.index,annualID,index,patch_name,uniqID,.geo))
# removes patches without a labeled change type
x <- x[y$ChangeType!="",]
y <- y[y$ChangeType!="",]
# filters out patches within the elevation mask
mask <- y$ChangeDesc
if (elevation_mask) {
  x <- x[mask!="Fully in mask",]
  y <- y[mask!="Fully in mask",]
}
# converts 'x' data to numeric and changes NA values to 0
x <- data.frame(sapply(x,as.numeric))
x[is.na(x)] <- 0
# extracts the disturbance labels from 'y'
y <- data.frame(y$ChangeType)
# relabels certain disturbances with their parent disturbance
if (group_post_with_parent) {
  y[y[,1]=="Post Fire Defoliation",] <- "Fire"
  y[y[,1]=="Post Tree Toppling",] <- "Tree Toppling"
  y[y[,1]=="Post Clearing",] <- "Clearing"
}
# relabels water disturbances with inter-annual variability
if (group_water_with_iav) {
   y[y[,1]=="Water",] <- "Inter-annual Variability"
}
# relabels high-elevation riparian disturbances with mass movement
if (riparian_to_mass_movement) {
  y[which(y[,1]=="Riparian"&x$elev_mean>914.4),] <- "Mass Movement"
}
# filters data by disturbance type
x <- x[y[,1]%in%disturbances,]
y <- y[y[,1]%in%disturbances,,drop=FALSE]
# removes predictors with more than 90% correlation with other variables
if (remove_correlated_vars) {
  correlations <- findCorrelation(cor(x),cutoff=0.9)
  x <- x[,-correlations]
}
```

#### (3) Train-Test Split
An important component of many supervised machine learning models is a 
train-test split. The purpose of a train-test-split is to purposefully withhold
some of our data from the training process, so we can test the model on the
data later. It's relatively easy to build a machine learning model that 
performs well on data it has already seen before, but it's harder to build one 
that does well on unseen data. This section divides our data into a training 
set and a testing set.\
\
There are a few nuances to our specific case. The first of these is that, since
some disturbance types have many more examples than others, it would not do to
simply take a random sample of, say, 75% of all of our data and call this the
training set. It is very likely that some disturbances would not be represented
in the training or testing set. To fix this issue, we perform stratified
sampling, which means we take a subset of each disturbance individually. This
ensures that each disturbance type will be represented. However, we would still
be dealing with extremely imbalanced data, as some disturbances have over 1000
examples and others fewer than 20. These discrepancies would be represented 
in the training and testing set, and we don't just want our model to just
perform well on the disturbances with the most examples. To fix this, we set
an upper limit on the number of examples from each disturbance that the model
can be trained with. This limit is arbitrarily chosen to be five times the
number of examples present in the least-frequent disturbance class. The final
nuance is that the user is able to specify, using the 'balance_testing_data'
variable, whether the testing data is balanced like the training data. We will
always train the random forest model with a balanced data set, but it is up to
the user whether to test the model on a balanced data set. One advantage of
not balancing the testing set is that, for disturbance classes with a large
number of examples, we can get a more representative classification accuracy.
However, an unbalanced testing set will lead to unhelpful omission error rates.
These are important considerations for assessing the model, but its is 
important to remember that balancing or not balancing the testing set will not
affect the inherent accuracy of a specific random forest model.
```{r train-test split}
# the maximum number of each disturbance to include in the train-test split
max_sample <- min(table(y))*5
# creates empty data frames that will hold the train and test sets
x_train <- data.frame()
y_train <- data.frame()
x_test <- data.frame()
y_test <- data.frame()
# loops through each disturbance type to perform stratified sampling
for (disturbance in disturbances) {
  # filters 'x' and 'y' by the current disturbance type
  x_subset <- x[y[,1]==disturbance,]
  y_subset <- y[y[,1]==disturbance,,drop=FALSE]
  # finds the number of examples of the current disturbance type
  samples <- nrow(x_subset)
  train_size <- 0
  if (samples > max_sample) {
    # if there are more disturbance examples than the maximum allowed, sets
    # the train set size to the maximum allowed * the split percent
    train_size <- floor(max_sample*train_test_split)
  } else {
    # if there are fewer disturbance examples than the maximum allowed, sets
    # the train set size to the number of disturbances * the split percent
    train_size <- floor(samples*train_test_split)
  }
  test_size <- 0
  if (balance_testing_data & samples > max_sample) {
    # if 'balance_testing_data' is TRUE and there are more disturbance examples
    # than the maximum allowed, sets the size of the test set to the maximum
    # allowed minus the size of the train set
    test_size <- max_sample-train_size
  } else {
    # if 'balance_testing_data' is FALSE, sets the size of the test set to
    # the number of disturbance examples minus the size of the train set
    test_size <- samples-train_size
  }
  # generates random, independent sets of indices for the train and test sets
  train_indices <- sample(1:samples,train_size,replace=FALSE)
  remaining_indices <- setdiff(1:samples,train_indices)
  test_indices <- sample(remaining_indices,test_size,replace=FALSE)
  # fills the empty data frames with the randomly selected disturbance examples
  x_train <- rbind(x_train,x_subset[train_indices,])
  y_train <- rbind(y_train,y_subset[train_indices,,drop=FALSE])
  x_test <- rbind(x_test,x_subset[test_indices,])
  y_test <- rbind(y_test,y_subset[test_indices,,drop=FALSE])
}
# converts the labels to factor type for random forest modeling
y_train <- factor(sapply(y_train,as.factor))
y_test <- factor(sapply(y_test,as.factor))
```

#### (4) Random Forest Training
We are now ready to train a random forest model. This block of code trains
an initial model using all of the available predictor variables and extracts
an ordered list of the most important. This importance metric for a given 
variable is an average of two quantities generated by the random forest 
algorithm: the mean decrease in gini and the mean decrease in accuracy. The
mean decrease in gini measures how the entropy, or variance, of a branch of
a tree decreases by splitting on a certain variable. The mean decrease in
accuracy measures the loss in model accuracy that would occur if a specific
predictor was removed from the model. The second block of code uses this 
importance ranking to train random forest models with successively fewer and
fewer predictor variables. The best model is found among these iterations.
```{r random forest training 1,fig.align='center'}
# trains a random forest model
forest <- randomForest(x=x_train,y=y_train,importance=TRUE,ntree=num_trees)
# creates importance metric from gini and accuracy values
gini <- forest$importance[,"MeanDecreaseGini"]
accuracy <- forest$importance[,"MeanDecreaseAccuracy"]
importance <- sort((gini+accuracy)/2,decreasing=TRUE)
# displays plots of predictor variables ranked by gini and accuracy values
varImpPlot(forest,sort=TRUE,n.var=15,main="Random Forest Variable Importance")
```

Now that we have a list of the most important variables, we train several
random forest models, dropping a number of the least predictive variables
each iteration. The random forest with the lowest average class error is saved.
Average class error is the average of each class's classification, or
comission, error. There are other metrics that could be used to decide which
model is the best, such as the average omission error or the total error.
The average class error was chosen because we want this model to perform well
on all classes, not just those with more examples. The average class error
for each iteration is shown in a chart.
```{r random forest training 2}
# a variable that will be used to compare successive random forest models
min_class_err <- Inf
# a variable that will store the number of predictors used in the best model
best_vars <- 0
# a variable that will store the best random forest
best_forest <- NULL
# the total number of predictor variables
num_vars <- ncol(x_train)
# how many predictors to drop each time a new random forest model is created
drop <- round(num_vars*0.1,0)
# creates a data frame to store the average class errors from each run
results <- data.frame()
# a variable to track where to store values in the 'results' array
row <- 1
# while there are variables left to create a random forest model with
while (num_vars > 1) {
  # stores the number of predictors used in this iteration
  results[row,1] <- num_vars
  # gets the names of the predictors used in this iteration
  vars <- names(importance[1:num_vars])
  # filters the train and test arrays by the predictors used in this iteration
  x_train_subset <- x_train[,vars]
  x_test_subset <- x_test[,vars]
  # trains a random forest model with a subset of predictor variables
  forest <- randomForest(x=x_train_subset,y=y_train,xtest=x_test_subset,
                         ytest=y_test,ntree=num_trees)
  # calculates the average class error of the random forest model
  avg_class_err <- mean(forest$confusion[,"class.error"])
  # stores the average class error
  results[row,2] <- round(avg_class_err*100,2)
  if (avg_class_err < min_class_err) {
    # if this is the run with the lowest average class error, store the
    # number of variables and save the random forest
    min_class_err <- avg_class_err
    best_vars <- length(vars)
    best_forest <- forest
  }
  # drop a certain number of predictors for the next random forest training
  num_vars <- num_vars-drop
  row <- row+1
}
# displays a chart showing the average class error for each random forest run
colnames(results) <- c("Number of Predictors","Average Class Error (%)")
DT::datatable(results,
              class='cell-border stripe hover',
              rownames=FALSE,
              caption="Average class error for each random forest",
              options=list(pageLength=nrow(results),dom="t",
                           columnDefs=list(list(className='dt-left',targets=0))))
```

#### (5) Random Forest Testing
We have found our best random forest model (with the top `r best_vars` 
predictors), so now let us look at the results. This block of code displays the 
distribution of the training set and the corresponding training confusion 
matrix, as well as the distribution of the testing set and the corresponding 
testing confusion matrix. We can use the comission and omission error rates to 
assess the accuracy of our forest.
```{r random forest testing 1}
# a function that creates and formats a confusion matrix for display
create_confusion_matrix <- function(confusion) {
  # rounds comission error to two decimal points
  confusion[,"class.error"] <- round(confusion[,"class.error"]*100,2)
  # changes a column name
  colnames(confusion) <- append(disturbance_labels,"Comission Error (%)")
  # calculates the omission error for each disturbance type
  omissions <- list()
  for (i in 1:num_dists) {
    omission <- round((1-(confusion[i,i]/sum(confusion[,i])))*100,2)
    omissions <- append(omissions,omission)
  }
  # adds the omission error to the confusion matrix
  confusion <- rbind(confusion,append(omissions,NA))
  rownames(confusion) <- append(disturbance_labels,"Omission Error (%)")
  return (confusion)
}
# calls the function above and calculates average error values for training set 
confusion_train <- create_confusion_matrix(best_forest$confusion)
comission_train <- round(mean(best_forest$confusion[,"class.error"])*100,2)
omission_train <- round(mean(as.numeric(confusion_train["Omission Error (%)",1:num_dists])),2)
# calls the function above and calculates average error values for testing set 
confusion_test <- create_confusion_matrix(best_forest$test$confusion)
comission_test <- round(mean(best_forest$test$confusion[,"class.error"])*100,2)
omission_test <- round(mean(as.numeric(confusion_test["Omission Error (%)",1:num_dists])),2)
```

This block of code displays the frequency distribution of the training set.
```{r random forest testing 2}
# displays the table described above
train_table <- as.data.frame(table(y_train))
colnames(train_table) <- c("Disturbance","Frequency")
DT::datatable(train_table,
              class='cell-border stripe hover',
              rownames=FALSE,
              caption="Frequency distribution of the training set",
              options=list(pageLength=nrow(train_table),dom="t"))
```

This block of code displays the frequency distribution of the testing set.
```{r random forest testing 3}
# displays the table described above
test_table <- as.data.frame(table(y_test))
colnames(test_table) <- c("Disturbance","Frequency")
DT::datatable(test_table,
              class='cell-border stripe hover',
              rownames=FALSE,
              caption="Frequency distribution of the testing set",
              options=list(pageLength=nrow(test_table),dom="t"))
```

This block of code displays the confusion matrix for the training set.\
The average comission error for the training set is: `r comission_train`%\
The average omission error for the training set is: `r omission_train`%\
```{r random forest testing 4}
# displays the confusion matrix described above
DT::datatable(confusion_train,
              class='cell-border stripe hover',
              caption="Confusion matrix for the training set",
              options=list(pageLength=nrow(confusion_train),dom="t",
                           order=list(classes="no-sort")))
```

This block of code displays the confusion matrix for the testing set.\
The average comission error for the testing set is: `r comission_test`%\
The average omission error for the testing set is: `r omission_test`%\
```{r random forest testing 5}
# displays the confusion matrix described above
DT::datatable(confusion_test,
              class='cell-border stripe hover',
              caption="Confusion matrix for the testing set",
              options=list(pageLength=nrow(confusion_test),dom="t",
                           order=list(classes="no-sort")))
```

Here we calculate the Pearson Correlation between the size of each 
disturbance's training set and respective testing performance.
```{r random forest testing 6}
train_examples <- (table(y_train))
comission_error <- 1-best_forest$test$confusion[,"class.error"]
pearson <- round(cor(train_examples,comission_error),2)
```
The Pearson Correlation between the size of each disturbance's training set and
respective testing performance is: `r pearson`\
\

#### (6) Confidence Evaluation
When our random forest makes a classification, it is taking the majority vote
among its `r num_trees` trees. We will call the percent of trees that vote for
the winning classification the model's "confidence" in that prediction. For
example, if `r num_trees/2` out of `r num_trees` trees vote for a certain 
disturbance class, we will say the model has a 50% confidence in that
classification. The code below evaluates, for each disturbance, how the
comission error over the testing set changes as we threshold by confidence.
Does the comission error rate decrease as we look only at classifications made
at higher and higher confidence levels? If that is the case, we can imagine
setting a comission error threshold, the classification error we are
comfortable with, and then finding the confidence level for each
disturbance at which the classification error is at or below the threshold. 
```{r confidence evaluation 1}
# converts the testing labels to a suitable data structure for analysis
y_test <- as.data.frame(y_test)
# creates a list of confidence thresholds
confidences <- seq(0,1,0.1)
# extracts the forest's voting distribution over the testing set
votes <- best_forest$test$votes
# creates a data frame to store error results
results <- data.frame(matrix(0,length(confidences),num_dists+1))
results[,1] <- round(confidences*100,0)
# creates a data frame to store confidence results
cutoffs <- data.frame(matrix(NA,num_dists,3))
# creates a logical vector that will be used to help store data
flags <- vector("logical",num_dists)
# for each confidence threshold
for (i in 1:length(confidences)) {
  confidence <- confidences[i]
  # for each disturbance
  for (j in 1:num_dists) {
    # filters the voting array by the current disturbance
    votes_subset <- votes[y_test==disturbances[j],]
    # filters the label array by the current disturbance
    y_test_subset <- y_test[y_test==disturbances[j],,drop=FALSE]
    # numerator and denominator used to calculate error
    num <- 0
    den <- 0
    # for each classified disturbance
    for (k in 1:nrow(votes_subset)) {
      # if the classification was made above a certain confidence level
      if (max(votes_subset[k,])>=confidence) {
        # increment the denominator
        den <- den+1
        # if the classification was correct
        if (names(which.max(votes_subset[k,]))[1]==y_test_subset[k,1]) {
          # increment the numerator
          num <- num+1
        }
      }
    }
    # calculate the disturbance's comission error at the current confidence
    error <- round((1-(num/den))*100,2)
    # store the error result
    results[i,j+1] <- error
    # if error is not null (if any classifications were made)
    if (!is.na(error)) {
      # if error is below the cutoff and a value hasn't been stored
      if (error <= error_cutoff & !flags[j]) {
        # store the confidence threshold
        cutoffs[j,1] <- round(confidence*100,0)
        # calculate the percent of the current disturbances classified
        percent_classified <- round(den/test_table$Frequency[j]*100,2)
        # store the percent of current disturbances classified
        cutoffs[j,2] <- percent_classified
        # stores the classification error
        cutoffs[j,3] <- error
        # flip the disturbance's flag so no more data will be stored
        flags[j] <- TRUE
      }
    }
  }
}
```

This block of code displays a chart showing the comission error for each
disturbance class broken down by confidence threshold. Blank spaces indicate
that no classifications for a certain class were made above a certain
confidence threshold, and thus there is no error rate. All values in percent.
```{r confidence evaluation 2}
# displays the chart described above
colnames(results) <- append("Confidence",disturbance_labels)
DT::datatable(results,
              class='cell-border stripe hover',
              caption="Comission error for each disturbance at confidence thresholds",
              rownames=FALSE,
              options=list(pageLength=nrow(results),dom="t",
                           order=list(classes="no-sort")))
```

This block of code displays a chart showing the confidence threshold at which
each disturbance achieves a comission error rate at or below the user chosen
value of `r error_cutoff`%. It also displays, at that confidence threshold,
what percent of a disturbance's testing set was classified. Blank spaces
indicate that a disturbance never achieved a suitable error rate.
```{r confidence evaluation 3}
# displays the chart described above
colnames(cutoffs) <- c("Confidence Threshold (%)","Classified at Threshold (%)",
                       "Comission Error (%)")
rownames(cutoffs) <- disturbance_labels
DT::datatable(cutoffs,
              class='cell-border stripe hover',
              caption=paste("What confidence threshold causes comission error 
                            to be below ",error_cutoff,"%",sep=""),
              options=list(pageLength=nrow(results),dom="t",
                           order=list(classes="no-sort")))
```

#### (7) Plotting Test Results
This section includes a few plots that will help us analyze our random forest
model. The first plot displays, for each disturbance type, at what confidence 
level correct classifications were made. Confidence will refer to the percent 
of trees in our forest that vote for a specific patch being a certain 
disturbance type. For example, if 80% of the trees in the forest vote that a
specific patch is an avalanche, we will say our forest is 80% confident that 
said patch is an avalanche. This first plot shows the distribution of these
confidence levels for correct classifications made by the model. All plots in
this section are analyzing the testing data, not the training.
```{r plotting test results 1,warning=FALSE,fig.align='center'}
y_test <- factor(sapply(y_test,as.factor))
# gets the distribution of random forest votes for every disturbance
votes <- best_forest$test$votes
# creates a data frame to store results
vote_percents <- data.frame()
# for every disturbance patch
for (i in 1:nrow(votes)) {
  # finds the disturbance type that received the most votes
  label <- colnames(votes)[which.max(votes[i,])]
  # if a correct classification was made
  if (label==y_test[i]) {
    # store the disturbance type and the percent of votes that type received
    row_num <- nrow(vote_percents)+1
    vote_percents[row_num,1] <- max(votes[i,])
    vote_percents[row_num,2] <- label
  }
}
# creates a data frame to store results
thresholds_graph <- data.frame(matrix(0,4,num_dists+1))
# sets the last column of the data frame equal to percentage bins
thresholds_graph[,num_dists+1] <- c("80-100%","60-80%","40-60%","0-40%")
# for each disturbance type
for (i in 1:num_dists) {
  # filters the 'vote_percents' array by disturbance type
  subset <- vote_percents[vote_percents[,2]==disturbances[i],1]
  # finds the number of disturbances after filtering
  len <- length(subset)
  # divides the correct classifications into bins based on confidence
  thresholds_graph[1,i] <- (length(subset[(subset>=.8&subset<=1)])/len)*100
  thresholds_graph[2,i] <- (length(subset[(subset>=.6&subset<.8)])/len)*100
  thresholds_graph[3,i] <- (length(subset[(subset>=.4&subset<.6)])/len)*100
  thresholds_graph[4,i] <- (length(subset[(subset>=0&subset<.4)])/len)*100
}
# sets the column names of the 'thresholds_graph' array
colnames(thresholds_graph) <- append(disturbance_labels,"Confidence")
# reorganizes array be suitable for plotting, pivots from wide to long
thresholds_graph <- pivot_longer(data=thresholds_graph,
                                 cols=all_of(disturbance_labels),
                                 names_to="Disturbance",
                                 values_to="Percent")
# plots the results
ggplot(data=thresholds_graph,
       aes(x=Disturbance,y=Percent,fill=Confidence),pattern="stripe")+
  geom_bar_pattern(position=position_stack(reverse=TRUE),stat="identity",
                   width=0.8,color="black",aes(pattern_angle=Confidence),
                   pattern_fill="white",pattern_color="white",
                   pattern_spacing=.02,pattern_density=0.05)+
  scale_pattern_angle_manual(values=c(0,45,90,-45))+
  labs(x="Disturbance Type",y="% of Disturbance Classifications",
       title="Random Forest Correct Classification Confidence Levels")+
  theme(plot.title=element_text(hjust=0.5))
```

This second plot shows, for each disturbance, how the classification (or
omission) error changes as we threshold by confidence level. The plot above
shows us the distribution of these confidence levels for correct
classifications, and this plot tells us if confidence levels matter. For
example, if we find that the classification errors for most classes drop
significantly if we only look at disturbance patches classified with above
50% confidence, we may choose to only accept results from the random forest
model that were classified above this threshold. It is also easy to imagine
setting an individual threshold for each disturbance type, depending on how
the classification error changes as we adjust the threshold.
```{r plotting test results 2,warning=FALSE,fig.align='center'}
# gets the classification error of each class with a confidence threshold
y_test <- as.data.frame(y_test)
thresholds <- c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)
threshold_data <- data.frame(matrix(0,length(thresholds),num_dists+1))
threshold_data[,num_dists+1] <- thresholds*100
# loops through the thresholds
for (i in 1:length(thresholds)) {
  threshold <- thresholds[i]
  # loops through the disturbance types
  for (j in 1:num_dists) {
    vote_subset <- votes[y_test==disturbances[j],]
    predicted_subset <- best_forest$test$predicted[y_test==disturbances[j]]
    y_test_subset <- y_test[y_test==disturbances[j],,drop=FALSE]
    num <- 0
    den <- 0
    # finds the percent of a disturbance type correctly classified with a
    # confidence above the current threshold
    for (k in 1:nrow(vote_subset)) {
      if (max(vote_subset[k,]) >= threshold) {
        den <- den+1
        if (predicted_subset[k]==y_test_subset[k,1]) {
          num <- num+1
        }
      }
    }
    threshold_data[i,j] <- round((1-(num/den))*100,2)
  }
}
# plots the results
colnames(threshold_data) <- append(disturbance_labels,"Threshold")
threshold_data <- pivot_longer(data=threshold_data,cols=all_of(disturbance_labels),
                                 names_to="Disturbance",values_to="Percent")
points <- c(0,1,2,5,15,16,17,18,3,4,12,13,8)
point_subset <- points[1:num_dists]
ggplot(data=threshold_data,aes(x=Threshold,y=Percent,group=Disturbance))+
  geom_line(aes(color=Disturbance),linewidth=1)+
  geom_point(aes(shape=Disturbance,color=Disturbance),size=3)+
  scale_shape_manual(values=c(0,1,2,5,15,16,17,18))+
  xlim(0,100)+ylim(0,100)+theme(plot.title=element_text(hjust=0.5))+
  labs(x="Confidence Threshold (%)",y="Class Error (%)",
       title="RF Classification Error by Confidence")
```

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
